# -*- coding: utf-8 -*-
"""A_53_AI_Practical_8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CfveRzDRxi5FMyXu6aYdKKdEUn3qd538

# Artificial Intelligence Lab
# Practical - 08

<hr>

### Name : Prathamesh Rajbhoj
### Roll : 53
### Batch : A-3
"""

def activation_function(value):

  # using relu as Activation Function

  if value > 0:
      value = 1
  else:
      value = -1

  return value

# def Perceptron(inputs, output, learning_rate=0.1, max_iterations=1000):

#   ans = 0

#   random1 = random.random()
#   random2 = random.random()
#   random3 = random.random()

#   random1 = (random1*10)
#   random2 = (random2*10)
#   random3 = (random3*10)

#   w = [1.1, 1.1]
#   b = 0

#   for i in range(max_iterations):

#     error = 0

#     for i in range(len(inputs)):

#       ans = ans + 1

#       input = inputs[i]
#       actual = output[i]

#       x1 = input[0]
#       x2 = input[1]
#       w1 = w[0]
#       w2 = w[1]


#       calculated = w1*x1 + w2*x2 + b

#       calculated = activation_function(calculated)

#       if(calculated == actual):
#         continue

#       i=i-1

#       delta = actual - calculated

#       error = error + 1

#       w[0] += learning_rate * delta * x1
#       w[1] += learning_rate * delta * x2
#       b += learning_rate * delta

#     if(error == 0):
#       return w, b, int(ans/len(inputs))

#   return w, b, int(ans/len(inputs))

# import random

# INPUTS = [
#     [0,0],
#     [0,1],
#     [1,0],
#     [1,1],
# ]

# OUTPUT_AND_GATE = [0, 0, 0, 1]
# OUTPUT_OR_GATE = [0, 1, 1, 1]
# OUTPUT_NAND_GATE = [1, 1, 1, 0]
# OUTPUT_NOR_GATE = [1, 0, 0, 0]

# w_and, b_and, iter_and = Perceptron(INPUTS, OUTPUT_AND_GATE)
# print(f'AND gate : {iter_and} iterations')
# print(w_and, b_and, iter_and, "\n\n")



# w_or, b_or, iter_or = Perceptron(INPUTS, OUTPUT_OR_GATE)
# print(f'OR gate : {iter_or} iterations')
# print(w_or, b_or, iter_or, "\n\n")



# w_nand, b_nand, iter_nand = Perceptron(INPUTS, OUTPUT_NAND_GATE)
# print(f'NAND gate : {iter_nand} iterations')
# print(w_nand, b_nand, iter_nand, "\n\n")




# w_nor, b_nor, iter_nor = Perceptron(INPUTS, OUTPUT_NOR_GATE)
# print(f'NOR gate : {iter_nor} iterations')
# print(w_nor, b_nor, iter_nor, "\n\n")

def Perceptron(inputs, output, learning_rate=0.5, max_iterations=1000):

  ans = 0

  w = [0.3, -0.2, 0.1]
  b = -0.5

  for i in range(max_iterations):

    error = 0

    for i in range(len(inputs)):

      ans = ans + 1

      input = inputs[i]
      actual = output[i]

      x1 = input[0]
      x2 = input[1]
      x3 = input[2]

      w1 = w[0]
      w2 = w[1]
      w3 = w[2]


      calculated = w1*x1 + w2*x2 + w3*x3 + b

      calculated = activation_function(calculated)

      if(calculated == actual):
        print(i, "no error")
        continue

      i=i-1

      delta = actual - calculated

      error = error + 1

      w[0] += learning_rate * delta * x1
      w[1] += learning_rate * delta * x2
      w[2] += learning_rate * delta * x3
      b += learning_rate * delta

      print(i, w, b)

    if(error == 0):
      return w, b, int(ans/len(inputs))

  return w, b, int(ans/len(inputs))

import random

INPUTS = [
    [-1, -1, -1],
    [-1, -1, 1],
    [-1, 1, -1],
    [-1, 1, 1],
]

OUTPUT_AND_GATE = [1, -1, -1, -1]

w_and, b_and, iter_and = Perceptron(INPUTS, OUTPUT_AND_GATE)
print(f'AND gate : {iter_and} iterations')
print(w_and, b_and, iter_and, "\n\n")

